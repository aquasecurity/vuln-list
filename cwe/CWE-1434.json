{
 "Description": "The product has a component that relies on a\n\t  generative AI/ML model configured with inference parameters that\n\t  produce an unacceptably high rate of erroneous or unexpected\n\t  outputs.",
 "ExtendedDescription": [
  "Generative AI/ML models, such as those used for text\n\t\tgeneration, image synthesis, and other creative tasks, rely on\n\t\tinference parameters that control model behavior, such as\n\t\ttemperature, Top P, and Top K.  These parameters affect the\n\t\tmodel's internal decision-making processes, learning rate, and\n\t\tprobability distributions. Incorrect settings can lead to\n\t\tunusual behavior such as text \"hallucinations,\" unrealistic\n\t\timages, or failure to converge during training. The impact of\n\t\tsuch misconfigurations can compromise the integrity of the\n\t\tapplication. If the results are used in security-critical\n\t\toperations or decisions, then this could violate the intended\n\t\tsecurity policy, i.e., introduce a vulnerability."
 ],
 "RelatedWeaknesses": {
  "RelatedWeakness": [
   {
    "Nature": "ChildOf",
    "CWEID": 440,
    "ViewID": 1000,
    "ChainID": 0,
    "Ordinal": "Primary"
   },
   {
    "Nature": "ChildOf",
    "CWEID": 665,
    "ViewID": 1000,
    "ChainID": 0,
    "Ordinal": ""
   },
   {
    "Nature": "CanPrecede",
    "CWEID": 684,
    "ViewID": 1000,
    "ChainID": 0,
    "Ordinal": ""
   },
   {
    "Nature": "PeerOf",
    "CWEID": 691,
    "ViewID": 1000,
    "ChainID": 0,
    "Ordinal": ""
   }
  ]
 },
 "WeaknessOrdinalities": {
  "WeaknessOrdinality": [
   {
    "Ordinality": "Primary",
    "Description": ""
   }
  ]
 },
 "ApplicablePlatforms": {
  "Language": [
   {
    "Name": "",
    "Class": "Not Language-Specific",
    "Prevalence": "Undetermined"
   }
  ],
  "OperatingSystem": null,
  "Architecture": [
   {
    "Name": "",
    "Class": "Not Architecture-Specific",
    "Prevalence": "Undetermined"
   }
  ],
  "Technology": [
   {
    "Name": "AI/ML",
    "Class": "",
    "Prevalence": "Undetermined"
   },
   {
    "Name": "",
    "Class": "Not Technology-Specific",
    "Prevalence": "Undetermined"
   }
  ]
 },
 "BackgroundDetails": {
  "BackgroundDetail": null
 },
 "AlternateTerms": {
  "AlternateTerm": null
 },
 "ModesOfIntroduction": {
  "Introduction": [
   {
    "Phase": "Build and Compilation",
    "Note": null
   },
   {
    "Phase": "Installation",
    "Note": null
   },
   {
    "Phase": "Patching and Maintenance",
    "Note": null
   }
  ]
 },
 "ExploitationFactors": {
  "ExploitationFactor": null
 },
 "LikelihoodOfExploit": "",
 "CommonConsequences": {
  "Consequence": [
   {
    "Scope": [
     "Integrity",
     "Other"
    ],
    "Impact": [
     "Varies by Context",
     "Unexpected State"
    ],
    "Likelihood": "",
    "Note": null,
    "ConsequenceID": ""
   },
   {
    "Scope": [
     "Other"
    ],
    "Impact": [
     "Alter Execution Logic",
     "Unexpected State",
     "Varies by Context"
    ],
    "Likelihood": "",
    "Note": null,
    "ConsequenceID": ""
   }
  ]
 },
 "DetectionMethods": {
  "DetectionMethod": [
   {
    "Method": "Automated Dynamic Analysis",
    "Description": null,
    "Effectiveness": "Moderate",
    "EffectivenessNotes": null,
    "DetectionMethodID": ""
   },
   {
    "Method": "Manual Dynamic Analysis",
    "Description": null,
    "Effectiveness": "Moderate",
    "EffectivenessNotes": null,
    "DetectionMethodID": ""
   }
  ]
 },
 "PotentialMitigations": {
  "Mitigation": [
   {
    "Phase": [
     "Implementation",
     "System Configuration",
     "Operation"
    ],
    "Strategy": "",
    "Description": null,
    "Effectiveness": "",
    "EffectivenessNotes": null,
    "MitigationID": ""
   },
   {
    "Phase": [
     "Implementation",
     "System Configuration",
     "Operation"
    ],
    "Strategy": "",
    "Description": null,
    "Effectiveness": "",
    "EffectivenessNotes": null,
    "MitigationID": ""
   },
   {
    "Phase": [
     "Documentation"
    ],
    "Strategy": "",
    "Description": null,
    "Effectiveness": "",
    "EffectivenessNotes": null,
    "MitigationID": ""
   }
  ]
 },
 "DemonstrativeExamples": {
  "DemonstrativeExample": [
   {
    "TitleText": "",
    "IntroText": [
     "Assume the product offers an\n\t\t\tLLM-based AI coding assistant to help users to write code\n\t\t\tas part of an Integrated Development Environment (IDE).\n\t\t\tAssume the model has been trained on real-world code, and\n\t\t\tthe model behaves normally under its default settings.\n\t\t\tSuppose there is a default temperature of 1, with a range\n\t\t\tof temperature values from 0 (most deterministic) to\n\t\t\t2.",
     "Consider the following configuration."
    ],
    "BodyText": [
     "The problem is that the configuration contains a temperature\n\t\t\thyperparameter that is higher than the default. This significantly\n\t\t\tincreases the likelihood that the LLM will suggest a package that did\n\t\t\tnot exist at training time, a behavior sometimes referred to as\n\t\t\t\"package hallucination.\"  Note that other possible behaviors could\n\t\t\tarise from higher temperature, not just package hallucination.",
     "An adversary could anticipate which package names could be generated\n\t\t\tand create a malicious package. For example, it has been observed that\n\t\t\tthe same LLM might hallucinate the same package regularly. Any code\n\t\t\tthat is generated by the LLM, when run by the user, would download and\n\t\t\texecute the malicious package. This is similar to typosquatting.",
     "The risk could be reduced by lowering the temperature so that it\n\t\t\treduces the unpredictable outputs and has a better chance of staying\n\t\t\tmore in line with the training data. If the temperature is set too\n\t\t\tlow, then some of the power of the model will be lost, and it may be\n\t\t\tless capable of producing solutions for rarely-encountered problems\n\t\t\tthat are not reflected in the training data. However, if the\n\t\t\ttemperature is not set low enough, the risk of hallucinating package\n\t\t\tnames may still be too high.  Unfortunately, the \"best\" temperature\n\t\t\tcannot be determined a priori, and sufficient empirical testing is\n\t\t\tneeded.",
     "In addition to more restrictive temperature settings, consider adding\n\t\t\t  guardrails that test that independently verify any referenced package\n\t\t\t  to ensure that it exists, is not obsolete, and comes from a trusted\n\t\t\t  party.",
     "Note that reducing temperature does not entirely eliminate the risk of\n\t\t\t  package hallucination.  Even with very low temperatures or other\n\t\t\t  settings, there is still a small chance that a non-existent package\n\t\t\t  name will be generated."
    ],
    "ExampleCode": {
     "Items": [
      "",
      "\n\t\t\t\t\"model\": \"my-coding-model\",\n\t\t\t\t\"context_window\": 8192,\n\t\t\t\t\"max_output_tokens\": 4096,\n\t\t\t\t\"temperature\", 1.5,\n\t\t\t\t...\n\t\t\t  ",
      "",
      "\n\t\t\t\t...\n\t\t\t\t\"temperature\", 0.2,\n\t\t\t\t...\n\t\t\t  ",
      ""
     ],
     "Language": "JSON",
     "Nature": "Good"
    },
    "References": {
     "Reference": null
    },
    "DemonstrativeExampleID": ""
   }
  ]
 },
 "ObservedExamples": {
  "ObservedExample": null
 },
 "FunctionalAreas": {
  "FunctionalArea": null
 },
 "AffectedResources": {
  "AffectedResource": null
 },
 "TaxonomyMappings": {
  "TaxonomyMapping": null
 },
 "RelatedAttackPatterns": {
  "RelatedAttackPattern": null
 },
 "References": {
  "Reference": [
   {
    "ExternalReferenceID": "REF-1487",
    "Section": ""
   }
  ]
 },
 "MappingNotes": {
  "Usage": "Allowed",
  "Rationale": null,
  "Comments": null,
  "Reasons": {
   "Reason": [
    {
     "Type": "Acceptable-Use"
    }
   ]
  },
  "Suggestions": {
   "Suggestion": null
  }
 },
 "Notes": {
  "Note": [
   {
    "Items": null,
    "Type": "Research Gap"
   }
  ]
 },
 "ContentHistory": {
  "Submission": {
   "SubmissionName": "Lily Wong",
   "SubmissionOrganization": "MITRE",
   "SubmissionDate": "2024-06-28T00:00:00Z",
   "SubmissionVersion": "4.18",
   "SubmissionReleaseDate": "2025-09-09T00:00:00Z",
   "SubmissionComment": ""
  },
  "Modification": null,
  "Contribution": [
   {
    "ContributionName": "AI WG \"New Entry\" subgroup",
    "ContributionOrganization": "",
    "ContributionDate": "2025-02-28T00:00:00Z",
    "ContributionVersion": "4.18",
    "ContributionReleaseDate": "2025-09-09T00:00:00Z",
    "ContributionComment": "Participated in regular meetings from February to August 2025 to develop and refine most elements of this entry.",
    "Type": "Content"
   }
  ],
  "PreviousEntryName": null
 },
 "ID": 1434,
 "Name": "Insecure Setting of Generative AI/ML Model Inference Parameters",
 "Abstraction": "Base",
 "Structure": "Simple",
 "Status": "Draft",
 "Diagram": ""
}