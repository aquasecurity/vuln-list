{
  "Title": "Security update for slurm_20_11 and pdsh",
  "Tracking": {
    "ID": "SUSE-SU-2021:0773-1",
    "Status": "Final",
    "Version": "1",
    "InitialReleaseDate": "2021-03-12T13:59:12Z",
    "CurrentReleaseDate": "2021-03-12T13:59:12Z",
    "RevisionHistory": [
      {
        "Number": "1",
        "Date": "2021-03-12T13:59:12Z",
        "Description": "current"
      }
    ]
  },
  "Notes": [
    {
      "Text": "Security update for slurm_20_11 and pdsh",
      "Title": "Topic",
      "Type": "Summary"
    },
    {
      "Text": "This update for pdsh fixes the following issues:\n\n- Preparing pdsh for Slurm 20.11 (jsc#ECO-2412)\n- Simplify convoluted condition.\n    \n    This update for slurm fixes the following issues:\n    \n- Fix potential buffer overflows from use of unpackmem(). CVE-2020-27745 (bsc#1178890)\n- Fix potential leak of the magic cookie when sent as an argument to the xauth command. CVE-2020-27746 (bsc#1178891)\n- Add support for openPMIx also for Leap/SLE 15.0/1 (bsc#1173805).\n- Updated to 20.02.3 which fixes CVE-2020-12693 (bsc#1172004).\n- slurm-plugins will now also require pmix not only libpmix (bsc#1164326)\n- Removed autopatch as it doesn't work for the SLE-11-SP4 build.\n- Disable %arm builds as this is no longer supported.\n- pmix searches now also for libpmix.so.2 so that there is no dependency for devel package (bsc#1164386)\n- Update to version 20.02.0 (jsc#SLE-8491)\n  * Fix minor memory leak in slurmd on reconfig.\n  * Fix invalid ptr reference when rolling up data in the database.\n  * Change shtml2html.py to require python3 for RHEL8 support, and match man2html.py.\n  * slurm.spec - override 'hardening' linker flags to ensure RHEL8 builds in a usable manner.\n  * Fix type mismatches in the perl API.\n  * Prevent use of uninitialized slurmctld_diag_stats.\n  * Fixed various Coverity issues.\n  * Only show warning about root-less topology in daemons.\n  * Fix accounting of jobs in IGNORE_JOBS reservations.\n  * Fix issue with batch steps state not loading correctly when upgrading from 19.05.\n  * Deprecate max_depend_depth in SchedulerParameters and move it to DependencyParameters.\n  * Silence erroneous error on slurmctld upgrade when loading federation state.\n  * Break infinite loop in cons_tres dealing with incorrect tasks per tres request resulting in slurmctld hang.\n  * Improve handling of --gpus-per-task to make sure appropriate number of GPUs is assigned to job.\n  * Fix seg fault on cons_res when requesting --spread-job.\n\n- Move to python3 for everything but SLE-11-SP4\n  * For SLE-11-SP4 add a workaround to handle a python3 script (python2.7 compliant).\n\n  * sbatch - fix segfault when no newline at the end of a burst buffer file.\n  * Change scancel to only check job's base state when matching -t options.\n  * Save job dependency list in state files.\n  * cons_tres - allow jobs to be run on systems with root-less topologies.\n  * Restore pre-20.02pre1 PrologSlurmctld synchonization behavior to avoid various race conditions, and ensure proper batch job launch.\n  * Add new slurmrestd command/daemon which implements the Slurm REST API.\n\n- standard slurm.conf uses now also SlurmctldHost on all build targets (bsc#1162377)\n\n- start slurmdbd after mariadb (bsc#1161716)\n\n- Update to version 19.05.5 (jsc#SLE-8491)\n  * Includes security fixes CVE-2019-19727, CVE-2019-19728, CVE-2019-12838.\n  * Disable i586 builds as this is no longer supported.\n  * Create libnss_slurm package to support user and group resolution thru slurmstepd.\n\n- Update to v18.08.9 for fixing CVE-2019-19728 (bsc#1159692).\n  * Make Slurm compile on linux after sys/sysctl.h was deprecated.\n  * Install slurmdbd.conf.example with 0600 permissions to encourage secure use. CVE-2019-19727.\n  * srun - do not continue with job launch if --uid fails. CVE-2019-19728.\n\n- added pmix support jsc#SLE-10800 \n\n- Use --with-shared-libslurm to build slurm binaries using libslurm.\n- Make libslurm depend on slurm-config.\n\n- Fix ownership of /var/spool/slurm on new installations\n  and upgrade (bsc#1158696).\n\n- Fix permissions of slurmdbd.conf (bsc#1155784, CVE-2019-19727).\n- Fix %posttrans macro _res_update to cope with added newline\n  (bsc#1153259).\n\n- Add package slurm-webdoc which sets up a web server to provide\n  the documentation for the version shipped.\n\n- Move srun from 'slurm' to 'slurm-node': srun is required on the\n  nodes as well so sbatch will work. 'slurm-node' is a requirement\n  when 'slurm' is installed (bsc#1153095).\n\n- Updated to 18.08.8 for fixing (CVE-2019-12838, bsc#1140709, jsc#SLE-7341, jsc#SLE-7342)\n  * Update 'xauth list' to use the same 10000ms timeout as the other xauth commands.\n  * Fix issue in gres code to handle a gres cnt of 0.\n  * Don't purge jobs if backfill is running.\n  * Verify job is pending add/removing accrual time.\n  * Don't abort when the job doesn't have an association that was removed before the job was able to make it to the database.\n  * Set state_reason if select_nodes() fails job for QOS or Account.\n  * Avoid seg_fault on referencing association without a valid_qos bitmap.\n  * If Association/QOS is removed on a pending job set that job as ineligible.\n  * When changing a jobs account/qos always make sure you remove the old limits.\n  * Don't reset a FAIL_QOS or FAIL_ACCOUNT job reason until the qos or account changed.\n  * Restore 'sreport -T ALL' functionality.\n  * Correctly typecast signals being sent through the api.\n  * Properly initialize structures throughout Slurm.\n  * Sync 'numtask' squeue format option for jobs and steps to 'numtasks'.\n  * Fix sacct -PD to avoid CA before start jobs.\n  * Fix potential deadlock with backup slurmctld.\n  * Fixed issue with jobs not appearing in sacct after dependency satisfied.\n  * Fix showing non-eligible jobs when asking with -j and not -s.\n  * Fix issue with backfill scheduler scheduling tasks of an array when not the head job.\n  * accounting_storage/mysql - fix SIGABRT in the archive load logic.\n  * accounting_storage/mysql - fix memory leak in the archive load logic.\n  * Limit records per single SQL statement when loading archived data.\n  * Fix unnecessary reloading of job submit plugins.\n  * Allow job submit plugins to be turned on/off with a reconfigure.\n  * Fix segfault when loading/unloading Lua job submit plugin multiple times.\n  * Fix printing duplicate error messages of jobs rejected by job submit plugin.\n  * Fix printing of job submit plugin messages of het jobs without pack id.\n  * Fix memory leak in group_cache.c\n  * Fix jobs stuck from FedJobLock when requeueing in a federation\n  * Fix requeueing job in a federation of clusters with differing associations\n  * sacctmgr - free memory before exiting in 'sacctmgr show runaway'.\n  * Fix seff showing memory overflow when steps tres mem usage is 0.\n  * Upon archive file name collision, create new archive file instead of overwriting the old one to prevent lost records.\n  * Limit archive files to 50000 records per file so that archiving large databases will succeed.\n  * Remove stray newlines in SPANK plugin error messages.\n  * Fix archive loading events.\n  * In select/cons_res: Only allocate 1 CPU per node with the --overcommit and --nodelist options.\n  * Fix main scheduler from potentially not running through whole queue.\n  * cons_res/job_test - prevent a job from overallocating a node memory.\n  * cons_res/job_test - fix to consider a node's current allocated memory when testing a job's memory request.\n  * Fix issue where multi-node job steps on cloud nodes wouldn't finish cleaning up until the end of the job (rather than the end of the step).\n  * Fix issue with a 17.11 sbcast call to a 18.08 daemon.\n  * Add new job bit_flags of JOB_DEPENDENT.\n  * Make it so dependent jobs reset the AccrueTime and do not count against any AccrueTime limits.\n  * Fix sacctmgr --parsable2 output for reservations and tres.\n  * Prevent slurmctld from potential segfault after job_start_data() called for completing job.\n  * Fix jobs getting on nodes with 'scontrol reboot asap'.\n  * Record node reboot events to database.\n  * Fix node reboot failure message getting to event table.\n  * Don't write '(null)' to event table when no event reason exists.\n  * Fix minor memory leak when clearing runaway jobs.\n  * Avoid flooding slurmctld and logging when prolog complete RPC errors occur.\n  * Fix GCC 9 compiler warnings.\n  * Fix seff human readable memory string for values below a megabyte.\n  * Fix dump/load of rejected heterogeneous jobs.\n  * For heterogeneous jobs, do not count the each component against the QOS or association job limit multiple times.\n  * slurmdbd - avoid reservation flag column corruption with the use of newer flags, instead preserve the older flag fields that we can still fit in the smallint field, and discard the rest.\n  * Fix security issue in accounting_storage/mysql plugin on archive file loads by always escaping strings within the slurmdbd. CVE-2019-12838.\n  * Fix underflow causing decay thread to exit.\n  * Fix main scheduler not considering hetjobs when building the job queue.\n  * Fix regression for sacct to display old jobs without a start time.\n  * Fix setting correct number of gres topology bits.\n  * Update hetjobs pending state reason when appropriate.\n  * Fix accounting_storage/filetxt's understanding of TRES.\n  * Set Accrue time when not enforcing limits.\n  * Fix srun segfault when requesting a hetjob with test_exec or bcast options.\n  * Hide multipart priorities log message behind Priority debug flag.\n  * sched/backfill - Make hetjobs sensitive to bf_max_job_start.\n  * Fix slurmctld segfault due to job's partition pointer NULL dereference.\n  * Fix issue with OR'ed job dependencies.\n  * Add new job's bit_flags of INVALID_DEPEND to prevent rebuilding a job's dependency string when it has at least one invalid and purged dependency.\n  * Promote federation unsynced siblings log message from debug to info.\n  * burst_buffer/cray - fix slurmctld SIGABRT due to illegal read/writes.\n  * burst_buffer/cray - fix memory leak due to unfreed job script content.\n  * node_features/knl_cray - fix script_argv use-after-free.\n  * burst_buffer/cray - fix script_argv use-after-free.\n  * Fix invalid reads of size 1 due to non null-terminated string reads.\n  * Add extra debug2 logs to identify why BadConstraints reason is set.\n\n- Do not build hdf5 support where not available.\n\n- Add support for version updates on SLE: Update packages to a later version than the version supported originally on SLE will receive a version string in their package name.\n\n- added the hdf5 job data gathering plugin\n\n- Add backward compatibility with SLE-11 SP4\n\n- Update to version 18.08.05:\n  * Add mitigation for a potential heap overflow on 32-bit systems in xmalloc. (CVE-2019-6438, bsc#1123304)\n- Fix fallout from 750cc23ed for CVE-2019-6438.\n\n- Update to 18.08.04, with following highlights\n  * Fix message sent to user to display preempted instead of time limit when\n    a job is preempted.\n  * Fix memory leak when a failure happens processing a nodes gres config.\n  * Improve error message when failures happen processing a nodes gres config.\n  * Don't skip jobs in scontrol hold.\n  * Allow --cpu-bind=verbose to be used with SLURM_HINT environment variable.\n  * Enhanced handling for runaway jobs\n  * cons_res: Delay exiting cr_job_test until after cores/cpus are calculated\n    and distributed.\n  * Don't check existence of srun --prolog or --epilog executables when set to\n    'none' and SLURM_TEST_EXEC is used.\n  * Add 'P' suffix support to job and step tres specifications.\n  * Fix jobacct_gather/cgroup to work correctly when more than one task is\n    started on a node.\n  * salloc - set SLURM_NTASKS_PER_CORE and SLURM_NTASKS_PER_SOCKET in the\n    environment if the corresponding command line options are used.\n  * slurmd - fix handling of the -f flag to specify alternate config file\n    locations.\n  * Add SchedulerParameters option of bf_ignore_newly_avail_nodes to avoid\n    scheduling lower priority jobs on resources that become available during\n    the backfill scheduling cycle when bf_continue is enabled.\n  * job_submit/lua: Add several slurmctld return codes and add user/group info\n  * salloc/sbatch/srun - print warning if mutually exclusive options of --mem\n    and --mem-per-cpu are both set.\n\n- restarting services on update only when activated \n- added rotation of logs\n- Added backported patches which harden the pam module pam_slurm_adopt. (BOO#1116758)\n\n- Moved config man pages to a separate package: This way, they won't get installed on compute nodes.                                                                                                                                  \n\n- added correct link flags for perl bindings (bsc#1108671)\n  * perl:Switch is required by slurm torque wrappers\n\n- Fix Requires(pre) and Requires(post) for slurm-config and slurm-node.\n  This fixes issues with failing slurm user creation when installed during initial system installation. (bsc#1109373)\n\n- When using a remote shared StateSaveLocation, slurmctld needs to\n  be started after remote filesystems have become available.\n  Add 'remote-fs.target' to the 'After=' directive in slurmctld.service\n  (bsc#1103561).\n\n- Update to 17.11.8\n  * Fix incomplete RESPONSE_[RESOURCE|JOB_PACK]_ALLOCATION building path.\n  * Do not allocate nodes that were marked down due to the node not responding\n    by ResumeTimeout.\n  * task/cray plugin - search for 'mems' cgroup information in the file\n    'cpuset.mems' then fall back to the file 'mems'.\n  * Fix ipmi profile debug uninitialized variable.\n  * PMIx: fixed the direct connect inline msg sending.\n  * MYSQL: Fix issue not handling all fields when loading an archive dump.\n  * Allow a job_submit plugin to change the admin_comment field during\n    job_submit_plugin_modify().\n  * job_submit/lua - fix access into reservation table.\n  * MySQL - Prevent deadlock caused by archive logic locking reads.\n  * Don't enforce MaxQueryTimeRange when requesting specific jobs.\n  * Modify --test-only logic to properly support jobs submitted to more than\n    one partition.\n  * Prevent slurmctld from abort when attempting to set non-existing\n    qos as def_qos_id.\n  * Add new job dependency type of 'afterburstbuffer'. The pending job will be\n    delayed until the first job completes execution and it's burst buffer\n    stage-out is completed.\n  * Reorder proctrack/task plugin load in the slurmstepd to match that of\n    slurmd\n    and avoid race condition calling task before proctrack can introduce.\n  * Prevent reboot of a busy KNL node when requesting inactive features.\n  * Revert to previous behavior when requesting memory per cpu/node introduced\n    in 17.11.7.\n  * Fix to reinitialize previously adjusted job members to their original\n    value\n    when validating the job memory in multi-partition requests.\n  * Fix _step_signal() from always returning SLURM_SUCCESS.\n  * Combine active and available node feature change logs on one line rather\n    than one line per node for performance reasons.\n  * Prevent occasionally leaking freezer cgroups.\n  * Fix potential segfault when closing the mpi/pmi2 plugin.\n  * Fix issues with  --exclusive=[user|mcs] to work correctly\n    with preemption or when job requests a specific list of hosts.\n  * Make code compile with hdf5 1.10.2+\n  * mpi/pmix: Fixed the collectives canceling.\n  * SlurmDBD: improve error message handling on archive load failure.\n  * Fix incorrect locking when deleting reservations.\n  * Fix incorrect locking when setting up the power save module.\n  * Fix setting format output length for squeue when showing array jobs.\n  * Add xstrstr function.\n  * Fix printing out of --hint options in sbatch, salloc --help.\n  * Prevent possible divide by zero in _validate_time_limit().\n  * Add Delegate=yes to the slurmd.service file to prevent systemd from\n    interfering with the jobs' cgroup hierarchies.\n  * Change the backlog argument to the listen() syscall within srun to 4096\n    to match elsewhere in the code, and avoid communication problems at scale.\n\n  Fix race in the slurmctld backup controller which prevents it\n  to clean up allocations on nodes properly after failing over\n  (bsc#1084917).\n- Handled %license in a backward compatible manner.\n\n- Add a 'Recommends: slurm-munge' to slurm-slurmdbd.\n\n- Shield comments between script snippets with a %{!?nil:...} to\n  avoid them being interpreted as scripts - in which case the update\n  level is passed as argument (see chapter 'Shared libraries' in:\n  https://en.opensuse.org/openSUSE:Packaging_scriptlet_snippets)\n  (bsc#1100850). \n\n- Update from 17.11.5 to 17.11.7\n- Fix security issue in handling of username and gid fields\n  CVE-2018-10995 and bsc#1095508 what implied an \n  update from 17.11.5 to 17.11.7\n  Highlights of 17.11.6:\n  * CRAY - Add slurmsmwd to the contribs/cray dir\n  * PMIX - Added the direct connect authentication.\n  * Prevent the backup slurmctld from losing the active/available node\n    features list on takeover.\n  * Be able to force power_down of cloud node even if in power_save state.\n  * Allow cloud nodes to be recognized in Slurm when booted out of band.\n  * Numerous fixes - check 'NEWS' file.\n  Highlights of 17.11.7:\n  * Notify srun and ctld when unkillable stepd exits.\n  * Numerous fixes - check 'NEWS' file.\n  * Fixes daemoniziation in newly introduced slurmsmwd daemon.\n- Rename:\n  * remain in sync with commit messages which introduced that file\n\n- Avoid running pretrans scripts when running in an instsys:\n  there may be not much installed, yet. pretrans code should\n  be done in lua, this way, it will be executed by the rpm-internal\n  lua interpreter and not be passed to a shell which may not be\n  around at the time this scriptlet is run (bsc#1090292).\n\n- Add requires for slurm-sql to the slurmdbd package.\n\n- Package READMEs for pam and pam_slurm_adopt.\n- Use the new %%license directive for COPYING file.\n  Fix interaction with systemd: systemd expects that a \n  daemonizing process doesn't go away until the PID file\n  with it PID of the daemon has bee written (bsc#1084125).\n\n- Make sure systemd services get restarted only when all\n  packages are in a consistent state, not in the middle\n  of an 'update' transaction (bsc#1088693).\n  Since the %postun scripts that run on update are from\n  the old package they cannot be changed - thus we work\n  around the restart breakage.\n\n- fixed wrong log file location in slurmdbd.conf and \n  fixed pid location for slurmdbd and made slurm-slurmdbd\n  depend on slurm config which provides the dir /var/run/slurm\n  (bsc#1086859).\n\n- added comment for (bsc#1085606) \n\n- Fix security issue in accounting_storage/mysql plugin by always escaping strings within the slurmdbd. CVE-2018-7033 (bsc#1085240).\n- Update slurm to v17.11.5 (FATE#325451)\n  Highlights of 17.11:\n  * Support for federated clusters to manage a single work-flow \n    across a set of clusters.\n  * Support for heterogeneous job allocations (various processor types,\n    memory sizes, etc. by job component). Support for heterogeneous job\n    steps within a single MPI_COMM_WORLD is not yet supported for most\n    configurations.\n  * X11 support is now fully integrated with the main Slurm code. Remove\n    any X11 plugin configured in your plugstack.conf file to avoid errors\n    being logged about conflicting options.\n  * Added new advanced reservation flag of 'flex', which permits jobs\n    requesting the reservation to begin prior to the reservation's \n    start time and use resources inside or outside of the reservation.\n    A typical use case is to prevent jobs not explicitly requesting the\n    reservation from using those reserved resources rather than forcing\n    jobs requesting the reservation to use those resources in the time\n    frame reserved.\n  * The sprio command has been modified to report a job's priority\n    information for every partition the job has been submitted to.\n  * Group ID lookup performed at job submit time to avoid lookup on\n    all compute nodes. Enable with PrologFlags=SendGIDs configuration\n    parameter.\n  * Slurm commands and daemons dynamically link to libslurmfull.so\n    instead of statically linking. This dramatically reduces the\n    footprint of Slurm.\n  * In switch plugin, added plugin_id symbol to plugins and wrapped\n    switch_jobinfo_t with dynamic_plugin_data_t in interface calls\n    in order to pass switch information between clusters with different\n    switch types.\n  * Changed default ProctrackType to cgroup.\n  * Changed default sched_min_interval from 0 to 2 microseconds.\n  * Added new 'scontrol write batch_script ' command to fetch a job's\n    batch script. Removed the ability to see the script as part of the \n    'scontrol -dd   show job' command.\n  * Add new 'billing' TRES which allows jobs to be limited based on the\n    job's billable TRES calculated by the job's partition's\n    TRESBillingWeights.\n  * Regular user use of 'scontrol top' command is now disabled. Use the\n    configuration parameter 'SchedulerParameters=enable_user_top' to\n    enable that functionality. The configuration parameter\n    'SchedulerParameters=disable_user_top' will be silently ignored.\n  * Change default to let pending jobs run outside of reservation after\n    reservation is gone to put jobs in held state. Added \n    NO_HOLD_JOBS_AFTER_END reservation flag to use old default.\n    Support for PMIx v2.0 as well as UCX support.\n  * Remove plugins for obsolete MPI stacks:\n    - lam\n    - mpich1_p4\n    - mpich1_shmem\n    - mvapich\n  * Numerous fixes - check 'NEWS' file.\n  Replaced by sed script.\n- Fix some rpmlint warnings.\n\n- moved config files to slurm-config package (FATE#324574).\n\n- Moved slurmstepd and man page into slurm-node due to slurmd dependency\n- Moved config files into slurm-node\n- Moved slurmd rc scripts into slurm-node\n- Made slurm-munge require slurm-plugins instead of slurm itself\n  - slurm-node suggested slurm-munge, causing the whole slurm to be\n    installed. The slurm-plugins seems to be a more base class\n    (FATE#324574).\n\n- split up light wight slurm-node package for deployment on nodes\n  (FATE#324574).\n\n- Package so-versioned libs separately. libslurm is expected\n  to change more frequently and thus is packaged separately\n  from libpmi.\n\n- Updated to 17.02.9 to fix CVE-2017-15566 (bsc#1065697).\n   Changes in 17.0.9\n   * When resuming powered down nodes, mark DOWN nodes right after\n     ResumeTimeout\n    has been reached (previous logic would wait about one minute longer).\n   * Fix sreport not showing full column name for TRES Count.\n   * Fix slurmdb_reservations_get() giving wrong usage data when job's spanned\n     reservation that was modified.\n   * Fix sreport reservation utilization report showing bad data.\n   * Show all TRES' on a reservation in sreport reservation utilization report\n     by default.\n   * Fix sacctmgr show reservation handling 'end' parameter.\n   * Work around issue with sysmacros.h and gcc7 / glibc 2.25.\n   * Fix layouts code to only allow setting a boolean.\n   * Fix sbatch --wait to keep waiting even if a message timeout occurs.\n   * CRAY - If configured with NodeFeatures=knl_cray and there are non-KNL\n     nodes which include no features the slurmctld will abort without\n     this patch when attemping strtok_r(NULL).\n   * Fix regression in 17.02.7 which would run the spank_task_privileged as\n     part of the slurmstepd instead of it's child process.\n   * Fix security issue in Prolog and Epilog by always prepending SPANK_ to\n     all user-set environment variables. CVE-2017-15566.\n   Changes in 17.0.8:\n   * Add 'slurmdbd:' to the accounting plugin to notify message is from dbd\n    instead of local.\n   * mpi/mvapich - Buffer being only partially cleared. No failures observed.\n   * Fix for job  --switch option on dragonfly network.\n   * In salloc with  --uid option, drop supplementary groups before changing UID.\n   * jobcomp/elasticsearch - strip any trailing slashes from JobCompLoc.\n   * jobcomp/elasticsearch - fix memory leak when transferring generated buffer.\n   * Prevent slurmstepd ABRT when parsing gres.conf CPUs.\n   * Fix sbatch --signal to signal all MPI ranks in a step instead of just those\n     on node 0.\n   * Check multiple partition limits when scheduling a job that were previously\n     only checked on submit.\n   * Cray: Avoid running application/step Node Health Check on the external\n     job step.\n   * Optimization enhancements for partition based job preemption.\n   * Address some build warnings from GCC 7.1, and one possible memory leak if\n     /proc is inaccessible.\n   * If creating/altering a core based reservation with scontrol/sview on a\n     remote cluster correctly determine the select type.\n   * Fix autoconf test for libcurl when clang is used.\n   * Fix default location for cgroup_allowed_devices_file.conf to use correct\n     default path.\n   * Document NewName option to sacctmgr.\n   * Reject a second PMI2_Init call within a single step to prevent slurmstepd\n     from hanging.\n   * Handle old 32bit values stored in the database for requested memory\n     correctly in sacct.\n   * Fix memory leaks in the task/cgroup plugin when constraining devices.\n   * Make extremely verbose info messages debug2 messages in the task/cgroup\n     plugin when constraining devices.\n   * Fix issue that would deny the stepd access to /dev/null where GRES has a\n    'type' but no file defined.\n   * Fix issue where the slurmstepd would fatal on job launch if you have no\n     gres listed in your slurm.conf but some in gres.conf.\n   * Fix validating time spec to correctly validate various time formats.\n   * Make scontrol work correctly with job update timelimit [+|-]=.\n   * Reduce the visibily of a number of warnings in _part_access_check.\n   * Prevent segfault in sacctmgr if no association name is specified for\n     an update command.\n   * burst_buffer/cray plugin modified to work with changes in Cray UP05\n     software release.\n   * Fix job reasons for jobs that are violating assoc MaxTRESPerNode limits.\n   * Fix segfault when unpacking a 16.05 slurm_cred in a 17.02 daemon.\n   * Fix setting TRES limits with case insensitive TRES names.\n   * Add alias for xstrncmp() -- slurm_xstrncmp().\n   * Fix sorting of case insensitive strings when using xstrcasecmp().\n   * Gracefully handle race condition when reading /proc as process exits.\n   * Avoid error on Cray duplicate setup of core specialization.\n   * Skip over undefined (hidden in Slurm) nodes in pbsnodes.\n   * Add empty hashes in perl api's slurm_load_node() for hidden nodes.\n   * CRAY - Add rpath logic to work for the alpscomm libs.\n   * Fixes for administrator extended TimeLimit (job reason \u0026 time limit reset).\n   * Fix gres selection on systems running select/linear.\n   * sview: Added window decorator for maximize,minimize,close buttons for all\n     systems.\n   * squeue: interpret negative length format specifiers as a request to\n     delimit values with spaces.\n   * Fix the torque pbsnodes wrapper script to parse a gres field with a type\n     set correctly.\n- Fixed ABI version of libslurm.\n\n- Trim redundant wording in descriptions.\n\n- Updated to slurm 17-02-7-1\n  * Added python as BuildRequires\n  * Removed sched-wiki package\n  * Removed slurmdb-direct package\n  * Obsoleted sched-wiki and slurmdb-direct packages\n  * Removing Cray-specific files\n  * Added /etc/slurm/layout.d files (new for this version)\n  * Remove /etc/slurm/cgroup files from package\n  * Added lib/slurm/mcs_account.so\n  * Removed lib/slurm/jobacct_gather_aix.so\n  * Removed lib/slurm/job_submit_cnode.so\n- Created slurm-sql package\n- Moved files from slurm-plugins to slurm-torque package\n- Moved creation of /usr/lib/tmpfiles.d/slurm.conf into slurm.spec\n  * Removed tmpfiles.d-slurm.conf\n- Changed /var/run path for slurm daemons to /var/run/slurm\n  (FATE#324026).\n\n- Made tmpfiles_create post-install macro SLE12 SP2 or greater\n- Directly calling systemd-tmpfiles --create for before SLE12 SP2\n\n- Allows OpenSUSE Factory build as well\n- Removes unused .service files from project\n- Adds /var/run/slurm to /usr/lib/tmpfiles.d for boottime creation\n  * Patches upstream .service files to allow for /var/run/slurm path\n  * Modifies slurm.conf to allow for /var/run/slurm path\n\n- Move wrapper script mpiexec provided by slrum-torque to\n  mpiexec.slurm to avoid conflicts. This file is normally\n  provided by the MPI implementation (bsc#1041706). \n\n- Replace remaining ${RPM_BUILD_ROOT}s.\n- Improve description.\n- Fix up changelog.\n\n- Spec file: Replace 'Requires : slurm-perlapi' by\n  'Requires: perl-slurm = %{version}' (bsc#1031872).\n\n- Trim redundant parts of description. Fixup RPM groups.\n- Replace unnecessary %__ macro indirections;\n  replace historic $RPM_* variables by macros.\n\n- Use %slurm_u and %slurm_g macros defined at the beginning of the spec\n  file when adding the slurm user/group for consistency.\n- Define these macros to daemon,root for non-systemd.\n- For anything newer than Leap 42.1 or SLE-12-SP1 build OpenHPC compatible.\n\n- Updated to 16.05.8.1\n * Remove StoragePass from being printed out in the slurmdbd log at debug2\n   level.\n * Defer PATH search for task program until launch in slurmstepd.\n * Modify regression test1.89 to avoid leaving vestigial job. Also reduce\n    logging to reduce likelyhood of Expect buffer overflow.\n * Do not PATH search for mult-prog launches if LaunchParamters=test_exec is\n    enabled.\n * Fix for possible infinite loop in select/cons_res plugin when trying to\n    satisfy a job's ntasks_per_core or socket specification.\n * If job is held for bad constraints make it so once updated the job doesn't\n    go into JobAdminHeld.\n * sched/backfill - Fix logic to reserve resources for jobs that require a\n    node reboot (i.e. to change KNL mode) in order to start.\n * When unpacking a node or front_end record from state and the protocol\n    version is lower than the min version, set it to the min.\n * Remove redundant lookup for part_ptr when updating a reservation's nodes.\n * Fix memory and file descriptor leaks in slurmd daemon's sbcast logic.\n * Do not allocate specialized cores to jobs using the --exclusive option.\n * Cancel interactive job if Prolog failure with 'PrologFlags=contain' or\n   'PrologFlags=alloc' configured. Send new error prolog failure message to\n   the salloc or srun command as needed.\n * Prevent possible out-of-bounds read in slurmstepd on an invalid #! line.\n * Fix check for PluginDir within slurmctld to work with multiple directories.\n * Cancel interactive jobs automatically on communication error to launching\n   srun/salloc process.\n * Fix security issue caused by insecure file path handling triggered by the\n   failure of a Prolog script. To exploit this a user needs to anticipate or\n   cause the Prolog to fail for their job. CVE-2016-10030 (bsc#1018371).\n- Replace group/user add macros with function calls.\n- Fix array initialzation and ensure strings are always NULL terminated in\n-  pam_slurm.c (bsc#1007053).\n- Disable building with netloc support: the netloc API is part of the devel\n  branch of hwloc. Since this devel branch was included accidentally and has\n  been reversed since, we need to disable this for the time being.\n- Conditionalized architecture specific pieces to support non-x86 architectures\n  better.\n\n- Remove: unneeded 'BuildRequires:  python'\n- Add:\n  BuildRequires:  freeipmi-devel\n  BuildRequires:  libibmad-devel\n  BuildRequires:  libibumad-devel\n  so they are picked up by the slurm build.\n- Enable modifications from openHPC Project.\n- Enable lua API package build.\n- Add a recommends for slurm-munge to the slurm package:\n  This is way, the munge auth method is available and slurm\n  works out of the box.\n- Create /var/lib/slurm as StateSaveLocation directory.\n  /tmp is dangerous. \n\n- Create slurm user/group in preinstall script.\n\n- Keep %{_libdir}/libpmi* and %{_libdir}/mpi_pmi2* on SUSE.\n\n- Fix build with and without OHCP_BUILD define.\n- Fix build for systemd and non-systemd.\n\n- Updated to 16-05-5 - equvalent to OpenHPC 1.2.\n  * Fix issue with resizing jobs and limits not be kept track of correctly.\n  * BGQ - Remove redeclaration of job_read_lock.\n  * BGQ - Tighter locks around structures when nodes/cables change state.\n  * Make it possible to change CPUsPerTask with scontrol.\n  * Make it so scontrol update part qos= will take away a partition QOS from\n    a partition.\n  * Backfill scheduling properly synchronized with Cray Node Health Check.\n    Prior logic could result in highest priority job getting improperly\n    postponed.\n  * Make it so daemons also support TopologyParam=NoInAddrAny.\n  * If scancel is operating on large number of jobs and RPC responses from\n    slurmctld daemon are slow then introduce a delay in sending the cancel job\n    requests from scancel in order to reduce load on slurmctld.\n  * Remove redundant logic when updating a job's task count.\n  * MySQL - Fix querying jobs with reservations when the id's have rolled.\n  * Perl - Fix use of uninitialized variable in slurm_job_step_get_pids.\n  * Launch batch job requsting --reboot after the boot completes.\n  * Do not attempt to power down a node which has never responded if the\n    slurmctld daemon restarts without state.\n  * Fix for possible slurmstepd segfault on invalid user ID.\n  * MySQL - Fix for possible race condition when archiving multiple clusters\n    at the same time.\n  * Add logic so that slurmstepd can be launched under valgrind.\n  * Increase buffer size to read /proc/*/stat files.\n  * Remove the SchedulerParameters option of 'assoc_limit_continue', making it\n    the default value. Add option of 'assoc_limit_stop'. If 'assoc_limit_stop'\n    is set and a job cannot start due to association limits, then do not attempt\n    to initiate any lower priority jobs in that partition. Setting this can\n    decrease system throughput and utlization, but avoid potentially starving\n    larger jobs by preventing them from launching indefinitely.\n  * Update a node's socket and cores per socket counts as needed after a node\n    boot to reflect configuration changes which can occur on KNL processors.\n    Note that the node's total core count must not change, only the distribution\n    of cores across varying socket counts (KNL NUMA nodes treated as sockets by\n    Slurm).\n  * Rename partition configuration from 'Shared' to 'OverSubscribe'. Rename\n    salloc, sbatch, srun option from '--shared' to '--oversubscribe'. The old\n    options will continue to function. Output field names also changed in\n    scontrol, sinfo, squeue and sview.\n  * Add SLURM_UMASK environment variable to user job.\n  * knl_conf: Added new configuration parameter of CapmcPollFreq.\n  * Cleanup two minor Coverity warnings.\n  * Make it so the tres units in a job's formatted string are converted like\n    they are in a step.\n  * Correct partition's MaxCPUsPerNode enforcement when nodes are shared by\n    multiple partitions.\n  * node_feature/knl_cray - Prevent slurmctld GRES errors for 'hbm' references.\n  * Display thread name instead of thread id and remove process name in stderr\n    logging for 'thread_id' LogTimeFormat.\n  * Log IP address of bad incomming message to slurmctld.\n  * If a user requests tasks, nodes and ntasks-per-node and\n    tasks-per-node/nodes != tasks print warning and ignore ntasks-per-node.\n  * Release CPU 'owner' file locks.\n  * Update seff to fix warnings with ncpus, and list slurm-perlapi dependency\n    in spec file.\n  * Allow QOS timelimit to override partition timelimit when EnforcePartLimits\n    is set to all/any.\n  * Make it so qsub will do a 'basename' on a wrapped command for the output\n    and error files.\n  * Add logic so that slurmstepd can be launched under valgrind.\n  * Increase buffer size to read /proc/*/stat files.\n  * Prevent job stuck in configuring state if slurmctld daemon restarted while\n    PrologSlurmctld is running. Also re-issue burst_buffer/pre-load operation\n    as needed.\n  * Move test for job wait reason value of BurstBufferResources and\n    BurstBufferStageIn later in the scheduling logic.\n  * Document which srun options apply to only job, only step, or job and step\n    allocations.\n  * Use more compatible function to get thread name (\u003e= 2.6.11).\n  * Make it so the extern step uses a reverse tree when cleaning up.\n  * If extern step doesn't get added into the proctrack plugin make sure the\n    sleep is killed.\n  * Add web links to Slurm Diamond Collectors (from Harvard University) and\n    collectd (from EDF).\n  * Add job_submit plugin for the 'reboot' field.\n  * Make some more Slurm constants (INFINITE, NO_VAL64, etc.) available to\n    job_submit/lua plugins.\n  * Send in a -1 for a taskid into spank_task_post_fork for the extern_step.\n  * MYSQL - Sightly better logic if a job completion comes in with an end time\n    of 0.\n  * task/cgroup plugin is configured with ConstrainRAMSpace=yes, then set soft\n    memory limit to allocated memory limit (previously no soft limit was set).\n  * Streamline when schedule() is called when running with message aggregation\n    on batch script completes.\n  * Fix incorrect casting when [un]packing derived_ec on slurmdb_job_rec_t.\n  * Document that persistent burst buffers can not be created or destroyed using\n    the salloc or srun --bb options.\n  * Add support for setting the SLURM_JOB_ACCOUNT, SLURM_JOB_QOS and\n    SLURM_JOB_RESERVAION environment variables are set for the salloc command.\n    Document the same environment variables for the salloc, sbatch and srun\n    commands in their man pages.\n  * Fix issue where sacctmgr load cluster.cfg wouldn't load associations\n    that had a partition in them.\n  * Don't return the extern step from sstat by default.\n  * In sstat print 'extern' instead of 4294967295 for the extern step.\n  * Make advanced reservations work properly with core specialization.\n  * slurmstepd modified to pre-load all relevant plugins at startup to avoid\n    the possibility of modified plugins later resulting in inconsistent API\n    or data structures and a failure of slurmstepd.\n  * Export functions from parse_time.c in libslurm.so.\n  * Export unit convert functions from slurm_protocol_api.c in libslurm.so.\n  * Fix scancel to allow multiple steps from a job to be cancelled at once.\n  * Update and expand upgrade guide (in Quick Start Administrator web page).\n  * burst_buffer/cray: Requeue, but do not hold a job which fails the pre_run\n    operation.\n  * Insure reported expected job start time is not in the past for pending jobs.\n  * Add support for PMIx v2.\n  Required for FATE#316379.\n\n- Setting 'download_files' service to mode='localonly'\n  and adding source tarball. (Required for Factory).\n\n- version 15.08.7.1\n  * Remove the 1024-character limit on lines in batch scripts.\n    task/affinity: Disable core-level task binding if more CPUs required than\n    available cores.\n  * Preemption/gang scheduling: If a job is suspended at slurmctld restart or\n    reconfiguration time, then leave it suspended rather than resume+suspend.\n  * Don't use lower weight nodes for job allocation when topology/tree used.\n  * Don't allow user specified reservation names to disrupt the normal\n    reservation sequeuece numbering scheme.\n  * Avoid hard-link/copy of script/environment files for job arrays. Use the\n    master job record file for all tasks of the job array.\n    NOTE: Job arrays submitted to Slurm version 15.08.6 or later will fail if\n    the slurmctld daemon is downgraded to an earlier version of Slurm.\n  * In slurmctld log file, log duplicate job ID found by slurmd. Previously was\n    being logged as prolog/epilog failure.\n  * If a job is requeued while in the process of being launch, remove it's\n    job ID from slurmd's record of active jobs in order to avoid generating a\n    duplicate job ID error when launched for the second time (which would\n    drain the node).\n  * Cleanup messages when handling job script and environment variables in\n    older directory structure formats.\n  * Prevent triggering gang scheduling within a partition if configured with\n    PreemptType=partition_prio and PreemptMode=suspend,gang.\n  * Decrease parallelism in job cancel request to prevent denial of service\n    when cancelling huge numbers of jobs.\n  * If all ephemeral ports are in use, try using other port numbers.\n  * Prevent 'scontrol update job' from updating jobs that have already finished.\n  * Show requested TRES in 'squeue -O tres' when job is pending.\n  * Backfill scheduler: Test association and QOS node limits before reserving\n    resources for pending job.\n  * Many bug fixes.\n- Use source services to download package.\n- Fix code for new API of hwloc-2.0.\n- package netloc_to_topology where avialable.\n- Package documentation.\n\n- version 15.08.3\n  * Many new features and bug fixes. See NEWS file \n- update files list accordingly\n- fix wrong end of line in some files\n\n- version 14.11.8\n  * Many bug fixes. See NEWS file \n- update files list accordingly\n\n- add missing systemd requirements \n- add missing rclink\n\n- version 14.03.9\n  * Many bug fixes. See NEWS file\n- add systemd support \n\n- version 14.03.6\n  * Added support for native Slurm operation on Cray systems\n    (without ALPS).\n  * Added partition configuration parameters AllowAccounts,\n    AllowQOS, DenyAccounts and DenyQOS to provide greater control\n    over use.\n  * Added the ability to perform load based scheduling. Allocating\n    resources to jobs on the nodes with the largest number if idle\n    CPUs.\n  * Added support for reserving cores on a compute node for system\n    services (core specialization)\n  * Add mechanism for job_submit plugin to generate error message\n    for srun, salloc or sbatch to stderr.\n  * Support for Postgres database has long since been out of date\n    and problematic, so it has been removed entirely.  If you\n    would like to use it the code still exists in \u003c= 2.6, but will\n    not be included in this and future versions of the code.\n  * Added new structures and support for both server and cluster\n    resources.\n  * Significant performance improvements, especially with respect\n    to job array support. \n- update files list\n\n- update to version 2.6.7\n  * Support for job arrays, which increases performance and ease of\n    use for sets of similar jobs.\n  * Job profiling capability added to record a wide variety of job\n    characteristics for each task on a user configurable periodic\n    basis. Data currently available includes CPU use, memory use,\n    energy use, Infiniband network use, Lustre file system use, etc.\n  * Support for MPICH2 using PMI2 communications interface with much\n    greater scalability.\n  * Prolog and epilog support for advanced reservations.\n  * Much faster throughput for job step execution with --exclusive\n    option. The srun process is notified when resources become\n    available rather than periodic polling.\n  * Support improved for Intel MIC (Many Integrated Core) processor.\n  * Advanced reservations with hostname and core counts now supports\n    asymmetric reservations (e.g. specific different core count for\n    each node).\n  * External sensor plugin infrastructure added to record power\n    consumption, temperature, etc.\n  * Improved performance for high-throughput computing.\n  * MapReduce+ support (launches ~1000x faster, runs ~10x faster).\n  * Added 'MaxCPUsPerNode' partition configuration parameter. This\n    can be especially useful to schedule GPUs. For example a node\n    can be associated with two Slurm partitions (e.g. 'cpu' and\n    'gpu') and the partition/queue 'cpu' could be limited to only a\n    subset of the node's CPUs, insuring that one or more CPUs would\n    be available to jobs in the 'gpu' partition/queue.\n\n- version 2.5.7\n  * Fix for linking to the select/cray plugin to not give warning\n    about undefined variable.\n  * Add missing symbols to the xlator.h\n  * Avoid placing pending jobs in AdminHold state due to backfill\n    scheduler interactions with advanced reservation.\n  * Accounting - make average by task not cpu.\n  * POE - Correct logic to support poe option '-euidevice sn_all'\n    and '-euidevice sn_single'.\n  * Accounting - Fix minor initialization error.\n  * POE - Correct logic to support srun network instances count\n    with POE.\n  * POE - With the srun --launch-cmd option, report proper task\n    count when the --cpus-per-task option is used without the\n    --ntasks option.\n  * POE - Fix logic binding tasks to CPUs.\n  * sview - Fix race condition where new information could of\n    slipped past the node tab and we didn't notice.\n  * Accounting - Fix an invalid memory read when slurmctld sends\n    data about start job to slurmdbd.\n  * If a prolog or epilog failure occurs, drain the node rather\n    than setting it down and killing all of its jobs.\n  * Priority/multifactor - Avoid underflow in half-life calculation.\n  * POE - pack missing variable to allow fanout (more than 32\n    nodes)\n  * Prevent clearing reason field for pending jobs. This bug was\n    introduced in v2.5.5 (see 'Reject job at submit time ...').\n  * BGQ - Fix issue with preemption on sub-block jobs where a job\n    would kill all preemptable jobs on the midplane instead of just\n    the ones it needed to. \n  * switch/nrt - Validate dynamic window allocation size.\n  * BGQ - When --geo is requested do not impose the default\n    conn_types.\n  * RebootNode logic - Defers (rather than forgets) reboot request\n    with job running on the node within a reservation.\n  * switch/nrt - Correct network_id use logic. Correct support for\n    user sn_all and sn_single options.\n  * sched/backfill - Modify logic to reduce overhead under heavy\n    load.\n  * Fix job step allocation with --exclusive and --hostlist option.\n  * Select/cons_res - Fix bug resulting in error of 'cons_res: sync\n    loop not progressing, holding job #'\n  * checkpoint/blcr - Reset max_nodes from zero to NO_VAL on job\n    restart.\n  * launch/poe - Fix for hostlist file support with repeated host\n    names.\n  * priority/multifactor2 - Prevent possible divide by zero.\n    -- srun - Don't check for executable if --test-only flag is\n    used.\n  * energy - On a single node only use the last task for gathering\n    energy. Since we don't currently track energy usage per task \n    (only per step). Otherwise we get double the energy.\n\n- version 2.5.4\n  * Support for IntelÂ® Many Integrated Core (MIC) processors.\n  * User control over CPU frequency of each job step.\n  * Recording power usage information for each job.\n  * Advanced reservation of cores rather than whole nodes.\n  * Integration with IBM's Parallel Environment including POE (Parallel\n    Operating Environment) and NRT (Network Resource Table) API.\n  * Highly optimized throughput for serial jobs in a new\n    'select/serial' plugin.\n  * CPU load is information available\n  * Configurable number of CPUs available to jobs in each SLURM\n    partition, which provides a mechanism to reserve CPUs for use\n    with GPUs.\n\n- remore runlevel 4 from init script thanks to patch1 \n- fix self obsoletion of slurm-munge package\n- use fdupes to remove duplicates \n- spec file reformaing\n\n- put perl macro in a better within install section \n\n- enable numa on x86_64 arch only \n\n- add numa and hwloc support\n\n- fix perl module files list \n\n- use perl_process_packlist macro for the perl files cleanup\n- fix some summaries length\n- add cgoups directory and example the cgroup.release_common file\n\n- spec file cleanup \n\n- first package\n\n",
      "Title": "Details",
      "Type": "General"
    },
    {
      "Text": "The CVRF data is provided by SUSE under the Creative Commons License 4.0 with Attribution (CC-BY-4.0).",
      "Title": "Terms of Use",
      "Type": "Legal Disclaimer"
    },
    {
      "Text": "SUSE-2021-773,SUSE-SLE-Module-HPC-12-2021-773",
      "Title": "Patchnames",
      "Type": "Details"
    }
  ],
  "ProductTree": {
    "Relationships": [
      {
        "ProductReference": "libnss_slurm2_20_11-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "libpmi0_20_11-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "libslurm36-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-dshgroup-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-genders-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-machines-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-netgroup-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-slurm-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-slurm_18_08-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-slurm_20_02-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "pdsh-slurm_20_11-2.34-7.32.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "perl-slurm_20_11-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-auth-none-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-config-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-config-man-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-devel-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-doc-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-lua-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-munge-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-node-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-pam_slurm-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-plugins-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-slurmdbd-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-sql-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-sview-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-torque-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      },
      {
        "ProductReference": "slurm_20_11-webdoc-20.11.4-3.5.1",
        "RelatesToProductReference": "SUSE Linux Enterprise Module for HPC 12",
        "RelationType": "Default Component Of"
      }
    ]
  },
  "References": [
    {
      "URL": "https://www.suse.com/support/update/announcement/2021/suse-su-20210773-1/",
      "Description": "Link for SUSE-SU-2021:0773-1"
    },
    {
      "URL": "https://lists.suse.com/pipermail/sle-security-updates/2021-March/008484.html",
      "Description": "E-Mail link for SUSE-SU-2021:0773-1"
    },
    {
      "URL": "https://www.suse.com/support/security/rating/",
      "Description": "SUSE Security Ratings"
    },
    {
      "URL": "https://bugzilla.suse.com/1018371",
      "Description": "SUSE Bug 1018371"
    },
    {
      "URL": "https://bugzilla.suse.com/1065697",
      "Description": "SUSE Bug 1065697"
    },
    {
      "URL": "https://bugzilla.suse.com/1085240",
      "Description": "SUSE Bug 1085240"
    },
    {
      "URL": "https://bugzilla.suse.com/1095508",
      "Description": "SUSE Bug 1095508"
    },
    {
      "URL": "https://bugzilla.suse.com/1123304",
      "Description": "SUSE Bug 1123304"
    },
    {
      "URL": "https://bugzilla.suse.com/1140709",
      "Description": "SUSE Bug 1140709"
    },
    {
      "URL": "https://bugzilla.suse.com/1155784",
      "Description": "SUSE Bug 1155784"
    },
    {
      "URL": "https://bugzilla.suse.com/1159692",
      "Description": "SUSE Bug 1159692"
    },
    {
      "URL": "https://bugzilla.suse.com/1172004",
      "Description": "SUSE Bug 1172004"
    },
    {
      "URL": "https://bugzilla.suse.com/1178890",
      "Description": "SUSE Bug 1178890"
    },
    {
      "URL": "https://bugzilla.suse.com/1178891",
      "Description": "SUSE Bug 1178891"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2016-10030/",
      "Description": "SUSE CVE CVE-2016-10030 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2017-15566/",
      "Description": "SUSE CVE CVE-2017-15566 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2018-10995/",
      "Description": "SUSE CVE CVE-2018-10995 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2018-7033/",
      "Description": "SUSE CVE CVE-2018-7033 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2019-12838/",
      "Description": "SUSE CVE CVE-2019-12838 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2019-19727/",
      "Description": "SUSE CVE CVE-2019-19727 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2019-19728/",
      "Description": "SUSE CVE CVE-2019-19728 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2019-6438/",
      "Description": "SUSE CVE CVE-2019-6438 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2020-12693/",
      "Description": "SUSE CVE CVE-2020-12693 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2020-27745/",
      "Description": "SUSE CVE CVE-2020-27745 page"
    },
    {
      "URL": "https://www.suse.com/security/cve/CVE-2020-27746/",
      "Description": "SUSE CVE CVE-2020-27746 page"
    }
  ],
  "Vulnerabilities": [
    {
      "CVE": "CVE-2016-10030",
      "Description": "The _prolog_error function in slurmd/req.c in Slurm before 15.08.13, 16.x before 16.05.7, and 17.x before 17.02.0-pre4 has a vulnerability in how the slurmd daemon informs users of a Prolog failure on a compute node. That vulnerability could allow a user to assume control of an arbitrary file on the system. Any exploitation of this is dependent on the user being able to cause or anticipate the failure (non-zero return code) of a Prolog script that their job would run on. This issue affects all Slurm versions from 0.6.0 (September 2005) to present. Workarounds to prevent exploitation of this are to either disable your Prolog script, or modify it such that it always returns 0 (\"success\") and adjust it to set the node as down using scontrol instead of relying on the slurmd to handle that automatically. If you do not have a Prolog set you are unaffected by this issue.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "important"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2016-10030.html",
          "Description": "CVE-2016-10030"
        },
        {
          "URL": "https://bugzilla.suse.com/1018371",
          "Description": "SUSE Bug 1018371"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2017-15566",
      "Description": "Insecure SPANK environment variable handling exists in SchedMD Slurm before 16.05.11, 17.x before 17.02.9, and 17.11.x before 17.11.0rc2, allowing privilege escalation to root during Prolog or Epilog execution.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "important"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2017-15566.html",
          "Description": "CVE-2017-15566"
        },
        {
          "URL": "https://bugzilla.suse.com/1065697",
          "Description": "SUSE Bug 1065697"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2018-10995",
      "Description": "SchedMD Slurm before 17.02.11 and 17.1x.x before 17.11.7 mishandles user names (aka user_name fields) and group ids (aka gid fields).",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "moderate"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2018-10995.html",
          "Description": "CVE-2018-10995"
        },
        {
          "URL": "https://bugzilla.suse.com/1095508",
          "Description": "SUSE Bug 1095508"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2018-7033",
      "Description": "SchedMD Slurm before 17.02.10 and 17.11.x before 17.11.5 allows SQL Injection attacks against SlurmDBD.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "moderate"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2018-7033.html",
          "Description": "CVE-2018-7033"
        },
        {
          "URL": "https://bugzilla.suse.com/1085240",
          "Description": "SUSE Bug 1085240"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2019-12838",
      "Description": "SchedMD Slurm 17.11.x, 18.08.0 through 18.08.7, and 19.05.0 allows SQL Injection.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "moderate"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2019-12838.html",
          "Description": "CVE-2019-12838"
        },
        {
          "URL": "https://bugzilla.suse.com/1140709",
          "Description": "SUSE Bug 1140709"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2019-19727",
      "Description": "SchedMD Slurm before 18.08.9 and 19.x before 19.05.5 has weak slurmdbd.conf permissions.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "low"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2019-19727.html",
          "Description": "CVE-2019-19727"
        },
        {
          "URL": "https://bugzilla.suse.com/1155784",
          "Description": "SUSE Bug 1155784"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2019-19728",
      "Description": "SchedMD Slurm before 18.08.9 and 19.x before 19.05.5 executes srun --uid with incorrect privileges.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "moderate"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2019-19728.html",
          "Description": "CVE-2019-19728"
        },
        {
          "URL": "https://bugzilla.suse.com/1155784",
          "Description": "SUSE Bug 1155784"
        },
        {
          "URL": "https://bugzilla.suse.com/1159692",
          "Description": "SUSE Bug 1159692"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2019-6438",
      "Description": "SchedMD Slurm before 17.11.13 and 18.x before 18.08.5 mishandles 32-bit systems.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "critical"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2019-6438.html",
          "Description": "CVE-2019-6438"
        },
        {
          "URL": "https://bugzilla.suse.com/1123304",
          "Description": "SUSE Bug 1123304"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2020-12693",
      "Description": "Slurm 19.05.x before 19.05.7 and 20.02.x before 20.02.3, in the rare case where Message Aggregation is enabled, allows Authentication Bypass via an Alternate Path or Channel. A race condition allows a user to launch a process as an arbitrary user.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "moderate"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2020-12693.html",
          "Description": "CVE-2020-12693"
        },
        {
          "URL": "https://bugzilla.suse.com/1172004",
          "Description": "SUSE Bug 1172004"
        },
        {
          "URL": "https://bugzilla.suse.com/1173804",
          "Description": "SUSE Bug 1173804"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2020-27745",
      "Description": "Slurm before 19.05.8 and 20.x before 20.02.6 has an RPC Buffer Overflow in the PMIx MPI plugin.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "important"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2020-27745.html",
          "Description": "CVE-2020-27745"
        },
        {
          "URL": "https://bugzilla.suse.com/1178890",
          "Description": "SUSE Bug 1178890"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    },
    {
      "CVE": "CVE-2020-27746",
      "Description": "Slurm before 19.05.8 and 20.x before 20.02.6 exposes Sensitive Information to an Unauthorized Actor because xauth for X11 magic cookies is affected by a race condition in a read operation on the /proc filesystem.",
      "Threats": [
        {
          "Type": "Impact",
          "Severity": "low"
        }
      ],
      "References": [
        {
          "URL": "https://www.suse.com/security/cve/CVE-2020-27746.html",
          "Description": "CVE-2020-27746"
        },
        {
          "URL": "https://bugzilla.suse.com/1178891",
          "Description": "SUSE Bug 1178891"
        }
      ],
      "ProductStatuses": [
        {
          "Type": "Fixed",
          "ProductID": [
            "SUSE Linux Enterprise Module for HPC 12:libnss_slurm2_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libpmi0_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:libslurm36-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-dshgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-genders-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-machines-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-netgroup-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_18_08-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_02-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:pdsh-slurm_20_11-2.34-7.32.1",
            "SUSE Linux Enterprise Module for HPC 12:perl-slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-auth-none-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-config-man-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-devel-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-doc-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-lua-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-munge-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-node-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-pam_slurm-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-plugins-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-slurmdbd-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sql-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-sview-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-torque-20.11.4-3.5.1",
            "SUSE Linux Enterprise Module for HPC 12:slurm_20_11-webdoc-20.11.4-3.5.1"
          ]
        }
      ],
      "CVSSScoreSets": {}
    }
  ]
}