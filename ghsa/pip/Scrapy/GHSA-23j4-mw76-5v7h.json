{
  "Severity": "MODERATE",
  "UpdatedAt": "2024-05-14T20:14:50Z",
  "Package": {
    "Ecosystem": "PIP",
    "Name": "Scrapy"
  },
  "Advisory": {
    "DatabaseId": 246031,
    "Id": "GSA_kwCzR0hTQS0yM2o0LW13NzYtNXY3aM4AA8EP",
    "GhsaId": "GHSA-23j4-mw76-5v7h",
    "References": [
      {
        "Url": "https://github.com/scrapy/scrapy/security/advisories/GHSA-23j4-mw76-5v7h"
      },
      {
        "Url": "https://github.com/scrapy/scrapy/issues/457"
      },
      {
        "Url": "https://github.com/scrapy/scrapy/commit/36287cb665ab4b0c65fd53181c9a0ef04990ada6"
      },
      {
        "Url": "https://github.com/advisories/GHSA-23j4-mw76-5v7h"
      }
    ],
    "Identifiers": [
      {
        "Type": "GHSA",
        "Value": "GHSA-23j4-mw76-5v7h"
      }
    ],
    "Description": "### Impact\n\nScrapy was following redirects regardless of the URL protocol, so redirects were working for `data://`, `file://`, `ftp://`, `s3://`, and any other scheme defined in the `DOWNLOAD_HANDLERS` setting.\n\nHowever, HTTP redirects should only work between URLs that use the `http://` or `https://` schemes.\n\nA malicious actor, given write access to the start requests (e.g. ability to define `start_urls`) of a spider and read access to the spider output, could exploit this vulnerability to:\n- Redirect to any local file using the `file://` scheme to read its contents.\n- Redirect to an `ftp://` URL of a malicious FTP server to obtain the FTP username and password configured in the spider or project.\n- Redirect to any `s3://` URL to read its content using the S3 credentials configured in the spider or project.\n\nFor `file://` and `s3://`, how the spider implements its parsing of input data into an output item determines what data would be vulnerable. A spider that always outputs the entire contents of a response would be completely vulnerable, while a spider that extracted only fragments from the response could significantly limit vulnerable data.\n\n### Patches\n\nUpgrade to Scrapy 2.11.2.\n\n### Workarounds\n\nReplace the built-in retry middlewares (`RedirectMiddleware` and `MetaRefreshMiddleware`) with custom ones that implement the fix from Scrapy 2.11.2, and verify that they work as intended.\n\n### References\n\nThis security issue was reported by @mvsantos at https://github.com/scrapy/scrapy/issues/457.\n",
    "Origin": "UNSPECIFIED",
    "PublishedAt": "2024-05-14T20:14:49Z",
    "Severity": "MODERATE",
    "Summary": "Scrapy allows redirect following in protocols other than HTTP",
    "UpdatedAt": "2024-05-14T20:14:50Z",
    "WithdrawnAt": "",
    "CVSS": {
      "Score": 6.5,
      "VectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N"
    }
  },
  "Versions": [
    {
      "FirstPatchedVersion": {
        "Identifier": "2.11.2"
      },
      "VulnerableVersionRange": "\u003c 2.11.2"
    }
  ]
}