{
  "Severity": "HIGH",
  "UpdatedAt": "2025-03-21T17:40:54Z",
  "Package": {
    "Ecosystem": "PIP",
    "Name": "llama_index"
  },
  "Advisory": {
    "DatabaseId": 285379,
    "Id": "GSA_kwCzR0hTQS1qM3dyLW02eGgtNjRoZ84ABFrD",
    "GhsaId": "GHSA-j3wr-m6xh-64hg",
    "References": [
      {
        "Url": "https://nvd.nist.gov/vuln/detail/CVE-2024-12704"
      },
      {
        "Url": "https://github.com/run-llama/llama_index/commit/d1ecfb77578d089cbe66728f18f635c09aa32a05"
      },
      {
        "Url": "https://huntr.com/bounties/a0b638fd-21c6-4ba7-b381-6ab98472a02a"
      },
      {
        "Url": "https://github.com/advisories/GHSA-j3wr-m6xh-64hg"
      }
    ],
    "Identifiers": [
      {
        "Type": "GHSA",
        "Value": "GHSA-j3wr-m6xh-64hg"
      },
      {
        "Type": "CVE",
        "Value": "CVE-2024-12704"
      }
    ],
    "Description": "A vulnerability in the LangChainLLM class of the run-llama/llama_index repository, version v0.12.5, allows for a Denial of Service (DoS) attack. The stream_complete method executes the llm using a thread and retrieves the result via the get_response_gen method of the StreamingGeneratorCallbackHandler class. If the thread terminates abnormally before the _llm.predict is executed, there is no exception handling for this case, leading to an infinite loop in the get_response_gen function. This can be triggered by providing an input of an incorrect type, causing the thread to terminate and the process to continue running indefinitely.",
    "Origin": "UNSPECIFIED",
    "PublishedAt": "2025-03-20T12:32:43Z",
    "Severity": "HIGH",
    "Summary": "LlamaIndex Improper Handling of Exceptional Conditions vulnerability",
    "UpdatedAt": "2025-10-15T16:03:39Z",
    "WithdrawnAt": "",
    "CVSS": {
      "Score": 7.5,
      "VectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H"
    }
  },
  "Versions": [
    {
      "FirstPatchedVersion": {
        "Identifier": "0.12.6"
      },
      "VulnerableVersionRange": "\u003c 0.12.6"
    }
  ]
}