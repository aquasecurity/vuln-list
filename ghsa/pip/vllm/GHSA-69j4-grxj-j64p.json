{
  "Severity": "MODERATE",
  "UpdatedAt": "2025-11-20T21:26:27Z",
  "Package": {
    "Ecosystem": "PIP",
    "Name": "vllm"
  },
  "Advisory": {
    "DatabaseId": 322413,
    "Id": "GSA_kwCzR0hTQS02OWo0LWdyeGotajY0cM4ABOtt",
    "GhsaId": "GHSA-69j4-grxj-j64p",
    "References": [
      {
        "Url": "https://github.com/vllm-project/vllm/security/advisories/GHSA-69j4-grxj-j64p"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/pull/27205"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/commit/3ada34f9cb4d1af763fdfa3b481862a93eb6bd2b"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/chat_utils.py#L1602-L1610"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/openai/serving_engine.py#L809-L814"
      },
      {
        "Url": "https://github.com/advisories/GHSA-69j4-grxj-j64p"
      }
    ],
    "Identifiers": [
      {
        "Type": "GHSA",
        "Value": "GHSA-69j4-grxj-j64p"
      },
      {
        "Type": "CVE",
        "Value": "CVE-2025-62426"
      }
    ],
    "Description": "### Summary\nThe /v1/chat/completions and /tokenize endpoints allow a `chat_template_kwargs` request parameter that is used in the code before it is properly validated against the chat template. With the right `chat_template_kwargs` parameters, it is possible to block processing of the API server for long periods of time, delaying all other requests \n\n### Details\nIn serving_engine.py, the chat_template_kwargs are unpacked into kwargs passed to chat_utils.py `apply_hf_chat_template` with no validation on the keys or values in that chat_template_kwargs dict. This means they can be used to override optional parameters in the `apply_hf_chat_template` method, such as `tokenize`, changing its default from False to True.\n\nhttps://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/openai/serving_engine.py#L809-L814\n\nhttps://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/chat_utils.py#L1602-L1610\n\nBoth serving_chat.py and serving_tokenization.py call into this `_preprocess_chat` method of `serving_engine.py` and they both pass in `chat_template_kwargs`.\n\nSo, a `chat_template_kwargs` like `{\"tokenize\": True}` makes tokenization happen as part of applying the chat template, even though that is not expected. Tokenization is a blocking operation, and with sufficiently large input can block the API server's event loop, which blocks handling of all other requests until this tokenization is complete.\n\nThis optional `tokenize` parameter to `apply_hf_chat_template` does not appear to be used, so one option would be to just hard-code that to always be False instead of allowing it to be optionally overridden by callers. A better option may be to not pass `chat_template_kwargs` as unpacked kwargs but instead as a dict, and only unpack them after the logic in `apply_hf_chat_template` that resolves the kwargs against the chat template.\n\n### Impact\n\nAny authenticated user can cause a denial of service to a vLLM server with Chat Completion or Tokenize requests.\n\n### Fix\n\nhttps://github.com/vllm-project/vllm/pull/27205",
    "Origin": "UNSPECIFIED",
    "PublishedAt": "2025-11-20T21:26:24Z",
    "Severity": "MODERATE",
    "Summary": "vLLM vulnerable to DoS via large Chat Completion or Tokenization requests with specially crafted `chat_template_kwargs`",
    "UpdatedAt": "2025-11-20T21:26:27Z",
    "WithdrawnAt": "",
    "CVSS": {
      "Score": 6.5,
      "VectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H"
    }
  },
  "Versions": [
    {
      "FirstPatchedVersion": {
        "Identifier": "0.11.1"
      },
      "VulnerableVersionRange": "\u003e= 0.5.5, \u003c 0.11.1"
    }
  ]
}