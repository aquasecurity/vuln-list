{
  "Severity": "MODERATE",
  "UpdatedAt": "2025-04-15T21:21:06Z",
  "Package": {
    "Ecosystem": "PIP",
    "Name": "vllm"
  },
  "Advisory": {
    "DatabaseId": 289966,
    "Id": "GSA_kwCzR0hTQS1oZjNjLXd4ZzItNDlxOc4ABGyu",
    "GhsaId": "GHSA-hf3c-wxg2-49q9",
    "References": [
      {
        "Url": "https://github.com/mlc-ai/xgrammar/security/advisories/GHSA-389x-67px-mjg3"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/security/advisories/GHSA-hf3c-wxg2-49q9"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/pull/16283"
      },
      {
        "Url": "https://github.com/vllm-project/vllm/commit/cb84e45ac75b42ba6795145923e8eb323bb825ad"
      },
      {
        "Url": "https://github.com/advisories/GHSA-hf3c-wxg2-49q9"
      }
    ],
    "Identifiers": [
      {
        "Type": "GHSA",
        "Value": "GHSA-hf3c-wxg2-49q9"
      }
    ],
    "Description": "### Impact\n\nThis report is to highlight a vulnerability in XGrammar, a library used by the structured output feature in vLLM. The XGrammar advisory is here: https://github.com/mlc-ai/xgrammar/security/advisories/GHSA-389x-67px-mjg3\n\nThe [xgrammar](https://xgrammar.mlc.ai/docs/) library is the default backend used by vLLM to support structured output (a.k.a. guided decoding). Xgrammar provides a required, built-in cache for its compiled grammars stored in RAM. xgrammar is available by default through the OpenAI compatible API server with both the V0 and V1 engines.\n\nA malicious user can send a stream of very short decoding requests with unique schemas, resulting in an addition to the cache for each request. This can result in a Denial of Service by consuming all of the system's RAM.\n\nNote that even if vLLM was configured to use a different backend by default, it is still possible to choose xgrammar on a per-request basis using the `guided_decoding_backend` key of the `extra_body` field of the request with the V0 engine. This per-request choice is not available when using the V1 engine. \n### Patches\n\n* https://github.com/vllm-project/vllm/pull/16283\n\n### Workarounds\n\nThere is no way to workaround this issue in existing versions of vLLM other than preventing untrusted access to the OpenAI compatible API server.\n\n### References\n\n* https://github.com/mlc-ai/xgrammar/security/advisories/GHSA-389x-67px-mjg3",
    "Origin": "UNSPECIFIED",
    "PublishedAt": "2025-04-15T21:21:04Z",
    "Severity": "MODERATE",
    "Summary": "vLLM vulnerable to Denial of Service by abusing xgrammar cache",
    "UpdatedAt": "2025-04-15T21:21:06Z",
    "WithdrawnAt": "",
    "CVSS": {
      "Score": 6.5,
      "VectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H"
    }
  },
  "Versions": [
    {
      "FirstPatchedVersion": {
        "Identifier": "0.8.4"
      },
      "VulnerableVersionRange": "\u003e= 0.6.5, \u003c 0.8.4"
    }
  ]
}