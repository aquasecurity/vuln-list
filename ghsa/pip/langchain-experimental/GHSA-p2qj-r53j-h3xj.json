{
  "Severity": "CRITICAL",
  "UpdatedAt": "2024-09-19T18:23:48Z",
  "Package": {
    "Ecosystem": "PIP",
    "Name": "langchain-experimental"
  },
  "Advisory": {
    "DatabaseId": 260865,
    "Id": "GSA_kwCzR0hTQS1wMnFqLXI1M2otaDN4as4AA_sB",
    "GhsaId": "GHSA-p2qj-r53j-h3xj",
    "References": [
      {
        "Url": "https://nvd.nist.gov/vuln/detail/CVE-2024-46946"
      },
      {
        "Url": "https://docs.sympy.org/latest/modules/codegen.html"
      },
      {
        "Url": "https://gist.github.com/12end/68c0c58d2564ef4141bccd4651480820#file-cve-2024-46946-txt"
      },
      {
        "Url": "https://github.com/langchain-ai/langchain/releases/tag/langchain-experimental%3D%3D0.3.0"
      },
      {
        "Url": "https://github.com/advisories/GHSA-p2qj-r53j-h3xj"
      }
    ],
    "Identifiers": [
      {
        "Type": "GHSA",
        "Value": "GHSA-p2qj-r53j-h3xj"
      },
      {
        "Type": "CVE",
        "Value": "CVE-2024-46946"
      }
    ],
    "Description": "langchain_experimental (aka LangChain Experimental) 0.1.17 through 0.3.0 for LangChain allows attackers to execute arbitrary code through sympy.sympify (which uses eval) in LLMSymbolicMathChain. LLMSymbolicMathChain was introduced in fcccde406dd9e9b05fc9babcbeb9ff527b0ec0c6 (2023-10-05).",
    "Origin": "UNSPECIFIED",
    "PublishedAt": "2024-09-19T06:31:36Z",
    "Severity": "CRITICAL",
    "Summary": "LangChain Experimental Eval Injection vulnerability",
    "UpdatedAt": "2024-09-19T18:23:48Z",
    "WithdrawnAt": "",
    "CVSS": {
      "Score": 9.8,
      "VectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H"
    }
  },
  "Versions": [
    {
      "FirstPatchedVersion": {
        "Identifier": ""
      },
      "VulnerableVersionRange": "\u003e= 0.1.17, \u003c= 0.3.0"
    }
  ]
}